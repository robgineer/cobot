{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Cobot Documentation \u00b6 This page serves as a reference for the integration of a pneumatic Cobot (provided by Festo SE & Co. KG ) into MoveIt2 and ROS2 Control: https://github.com/robgineer/cobot . In oder to get a deep insight into this project and to use the Cobot for your custom tasks, follow the steps below. Check out the Quickstart Guide to get the Cobot running in Simulation or on its real hardware. Run some demos using the C++ / Python API or the MoveIt Task Constructor from the demo package . Get an overview of the MoveIt2 / ROS2 Control Configuration and the launch files . Get familiar with the Cobot Model and delve deeper into model creation in the Cobot Modelling Jupyter Notebook . Understand the custom Cobot control setup: Cobot Trajectory Controller / Cobot Hardware Interface . Learn how to evaluate your own planner config based on our planner evaluation . Calibrate your custom camera using the hand eye calibration implementation.","title":"Overview"},{"location":"#cobot-documentation","text":"This page serves as a reference for the integration of a pneumatic Cobot (provided by Festo SE & Co. KG ) into MoveIt2 and ROS2 Control: https://github.com/robgineer/cobot . In oder to get a deep insight into this project and to use the Cobot for your custom tasks, follow the steps below. Check out the Quickstart Guide to get the Cobot running in Simulation or on its real hardware. Run some demos using the C++ / Python API or the MoveIt Task Constructor from the demo package . Get an overview of the MoveIt2 / ROS2 Control Configuration and the launch files . Get familiar with the Cobot Model and delve deeper into model creation in the Cobot Modelling Jupyter Notebook . Understand the custom Cobot control setup: Cobot Trajectory Controller / Cobot Hardware Interface . Learn how to evaluate your own planner config based on our planner evaluation . Calibrate your custom camera using the hand eye calibration implementation.","title":"Cobot Documentation"},{"location":"cobot_configuration/","text":"MoveIt2 and ROS2 Control Configuration \u00b6 We plan trajectories and gripper actions with MoveIt2 and execute them using ROS2 control. MoveIt2 and ROS2 control contain several configuration files. These are located in the cobot_moveit_config directory. We will focus on the following files: moveit_controllers.yaml / moveit_custom_controllers.yaml ros2_controllers.yaml kinematics.yaml ompl_planning.yaml festo_cobot_model.sdf MoveIt Controller \u00b6 Files moveit_controllers.yaml as well as moveit_custom_controllers.yaml . Defines which controller manager is used. By default the moveit_simple_controller_manager/MoveItSimpleControllerManager is selected. Lists all ROS2 controllers that will communicate with MoveIt2 and their corresponding joints We are using the default MoveItSimpleControllerManager for our configuration. For an example of a custom MoveIt Controller Manager (that is not active in our project and serves only for reference purposes) refer Cobot Controller Manager . For the controllers we define the arm_group_controller of type FollowJointTrajectory and gripper_group_controller as well as vacuum_upper_joint_controller / vacuum_lower_joint_controller of type GripperCommand . Since we have a custom controller for the real Cobot, we reference the cobot_arm_group_controller in the moveit_custom_controllers.yaml . ROS2 Controllers: ros2_controllers.yaml \u00b6 Defines the update_rate (execution frequency of the controllers) and each controller type in detail. In our configuration we have the following controllers arm_group_controller: type: joint_trajectory_controller/JointTrajectoryController cobot_arm_group_controller: type: cobot_trajectory_controller/CobotTrajectoryController gripper_group_controller: type: position_controllers/GripperActionController vacuum_upper_joint_controller: type: position_controllers/GripperActionController vacuum_lower_joint_controller: type: position_controllers/GripperActionController joint_state_broadcaster: type: joint_state_broadcaster/JointStateBroadcaster While arm_group_controller us used for fake controls and gazebo , cobot_arm_group_controller is used for real Cobot control. A simplified graphical interpretation of the configuration is provided in the following. Inverse Kinematics Solver: kinematics.yaml \u00b6 Defines the plugin for Inverse Kinematics (IK). We have selected the default plugin but reduced the timeout and added solve_type: Speed in order to speed up IK. Note often TRAC_IK is mentioned as a more \"performant\" IK solver. In our setup this did not seem to apply. In order to use it install trac-ik , run sudo apt-get install ros-jazzy-trac-ik and set the kinematics_solver in kinematics.yaml to kinematics_solver: trac_ik_kinematics_plugin/TRAC_IKKinematicsPlugin Robot Semantics: festo_cobot_model.srdf \u00b6 Defines additional attributes to the URDF model (located in cobot_model/urdf ) such as the end effector and joint groups and enables deactivating collisions between joints.","title":"MoveIt2 and ROS2 Control Configuration"},{"location":"cobot_configuration/#moveit2-and-ros2-control-configuration","text":"We plan trajectories and gripper actions with MoveIt2 and execute them using ROS2 control. MoveIt2 and ROS2 control contain several configuration files. These are located in the cobot_moveit_config directory. We will focus on the following files: moveit_controllers.yaml / moveit_custom_controllers.yaml ros2_controllers.yaml kinematics.yaml ompl_planning.yaml festo_cobot_model.sdf","title":"MoveIt2 and ROS2 Control Configuration"},{"location":"cobot_configuration/#moveit-controller","text":"Files moveit_controllers.yaml as well as moveit_custom_controllers.yaml . Defines which controller manager is used. By default the moveit_simple_controller_manager/MoveItSimpleControllerManager is selected. Lists all ROS2 controllers that will communicate with MoveIt2 and their corresponding joints We are using the default MoveItSimpleControllerManager for our configuration. For an example of a custom MoveIt Controller Manager (that is not active in our project and serves only for reference purposes) refer Cobot Controller Manager . For the controllers we define the arm_group_controller of type FollowJointTrajectory and gripper_group_controller as well as vacuum_upper_joint_controller / vacuum_lower_joint_controller of type GripperCommand . Since we have a custom controller for the real Cobot, we reference the cobot_arm_group_controller in the moveit_custom_controllers.yaml .","title":"MoveIt Controller"},{"location":"cobot_configuration/#ros2-controllers-ros2_controllersyaml","text":"Defines the update_rate (execution frequency of the controllers) and each controller type in detail. In our configuration we have the following controllers arm_group_controller: type: joint_trajectory_controller/JointTrajectoryController cobot_arm_group_controller: type: cobot_trajectory_controller/CobotTrajectoryController gripper_group_controller: type: position_controllers/GripperActionController vacuum_upper_joint_controller: type: position_controllers/GripperActionController vacuum_lower_joint_controller: type: position_controllers/GripperActionController joint_state_broadcaster: type: joint_state_broadcaster/JointStateBroadcaster While arm_group_controller us used for fake controls and gazebo , cobot_arm_group_controller is used for real Cobot control. A simplified graphical interpretation of the configuration is provided in the following.","title":"ROS2 Controllers: ros2_controllers.yaml"},{"location":"cobot_configuration/#inverse-kinematics-solver-kinematicsyaml","text":"Defines the plugin for Inverse Kinematics (IK). We have selected the default plugin but reduced the timeout and added solve_type: Speed in order to speed up IK. Note often TRAC_IK is mentioned as a more \"performant\" IK solver. In our setup this did not seem to apply. In order to use it install trac-ik , run sudo apt-get install ros-jazzy-trac-ik and set the kinematics_solver in kinematics.yaml to kinematics_solver: trac_ik_kinematics_plugin/TRAC_IKKinematicsPlugin","title":"Inverse Kinematics Solver: kinematics.yaml"},{"location":"cobot_configuration/#robot-semantics-festo_cobot_modelsrdf","text":"Defines additional attributes to the URDF model (located in cobot_model/urdf ) such as the end effector and joint groups and enables deactivating collisions between joints.","title":"Robot Semantics: festo_cobot_model.srdf"},{"location":"cobot_hardware/","text":"Cobot Hardware Interface \u00b6 Manipulation of the Cobot with MoveIt2 and ROS2. This package encapsulates the Festo Cobot API into a ROS2 control hardware interface. The Cobot is controlled physically with this module. The package is hosted on the GitLab server of the Esslingen University repo . Hence, in order to control the Cobot, you need to be connected to the HS Esslingen VPN. Background: the package contains Festo proprietary implementations and binaries for the communication with the Cobot that were not released to the public. Therefore, this code is hosted on the Esslingen University GitLab while the remaining project is hosted on a public GitHub repository. Package Contents \u00b6 \u251c\u2500\u2500 boost_1_65_0 | \u251c\u2500\u2500 config \u2502 \u2514\u2500\u2500 festo_node_config.ini ... \u251c\u2500\u2500 lib \u2502 \u2514\u2500\u2500 libopc_ua.so | \u251c\u2500\u2500 open62541 | \u2514\u2500\u2500 src boost_1_65_0 contains an archive of the Boost version required for the build of libopc_ua.so lib contains a proprietary (by Festo) library extending the OPC-UA implementation open62541 an open source OPC-UA implementation src contains the implementation of the ROS2 controller cobot_hardware.cpp and the Cobot API robot_controller_comm.cpp Implementation overview \u00b6 We have two options to pass commands to the Cobot: single point (the final point of a trajectory, via SetRobotWaypoint ) or a list of points (via ExecuteInstructionList , representing a path). We cannot send actual trajectories or fine grained position commands as the SPS of the Cobot takes care of the trajectory (and we cannot by-pass the SPS). This, unfortunately, does not align with the ROS2 control principles. A basic overview of the trajectory generation and control is provided in the following: MoveIt Planning (OMPL) \u2192 Generates a collision free path \u2193 Time Parameterization Plugin (Iterative / TOTG) \u2192 Adds time (making it an actual trajectory) \u2193 Final RobotTrajectory message \u2193 MoveIt Simple Controller Manager \u2193 ros2_control controller \u2193 HardwareInterface (this package) Now, the ROS2 controller is designed to send position commands to the hardware in real time; which is not suitable for the Cobot. In theory, we could execute SetRobotWaypoint for each position. This, however is not feasible as the resulting movement of the Cobot becomes jerky. The Cobot will interpret each point as a trajectory and will pause in between. In previous implementations, we simply sent the final trajectory point to the Cobot (using a custom MoveItControllerManager, the CobotControllerManager ). Although this resulted in smooth movements of the Cobot arm, the resulting trajectory was calculated by the SPS. We therefore had no influence on the trajectory and collisions that were considered within trajectory planning were obsolete. Hence, even with a camera attached, the Cobot would have been blind. In order to overcome this limitation we use the instruction list mechanism, where we pass a list of points to the Cobot ( ExecuteInstructionList ). The benefits of this mechanism is an underlying smoothing of the points. Details on the smoothing implementation are provided in chapter 2.19 of the Festo Robotics Manual (refer GitLab repo ). Note: the list is nothing other than a path as we do not have an influence on the timing (controlled by the SPS). Since more than just one point is now send to this hardware interface, we have implemented a custom ROS2 controller (CobotTrajectoryController), that send out either the last point of a trajectory or the entire trajectory to this interface. Note: ROS2 control implies using single joint commands (declared as double) and since a full trajectory was therefore difficult to be passed, we introduced another communication feature: a singleton realtime buffer. Implementation details \u00b6 The following paragraphs serve as a reference on the major implementation details of the hardware interface. CMakeLists.txt \u00b6 ... a rather special one. It includes an ExternalProject : an older boost library version, that is required for libopc_ua.so . Hence, we extract the archive of the boost library, build it and link it to libopc_ua.so . This way the lib is satisfied with the older boost version and the remaining project references the system-wide boost version. ExternalProject_Add(boost165 URL file://${BOOST_ARCHIVE} CONFIGURE_COMMAND ./bootstrap.sh --with-libraries=system,filesystem --prefix=${BOOST_INSTALL_DIR} BUILD_COMMAND ./b2 link=shared install -j4 BUILD_IN_SOURCE 1 INSTALL_COMMAND ./b2 install ) #... set(Boost_INCLUDE_DIRS ${BOOST_INCLUDE_DIR}) set(Boost_LIBRARIES_165 ${BOOST_LIBRARY_DIR}/libboost_system.so ${BOOST_LIBRARY_DIR}/libboost_filesystem.so ) set_target_properties(opc-ua-api PROPERTIES IMPORTED_LOCATION \"${CMAKE_SOURCE_DIR}/lib/libopc_ua.so\" INTERFACE_INCLUDE_DIRECTORIES \"${CMAKE_SOURCE_DIR}/include\" INTERFACE_LINK_LIBRARIES \"${Boost_LIBRARIES_165}\" ) Shared Trajectory Buffer \u00b6 Implemented as a singleton real-time buffer => accessible from different sources in real-time (enabling thread-safety). It is required to pass full trajectories from the custom ROS2 controller ( cobot_trajectory_controller ) to the hardware interface. Using this data structure, we by-pass the hardware commands for the arm_group that are filled by the standard ROS2 controllers and use the full trajectory instead. cobot_hardware.cpp \u00b6 Implements the following methods // initialize size of internal arrays on_init(...); // initiate communication with the Cobot on_activate(...); // send out a list of readable states for the controllers export_state_interfaces(); // send out a list of commands for the controllers export_command_interfaces(); // read states of Cobot joints read(...); // write commands to Cobot joints write(...); read(...) \u00b6 For each joint, we read its current state using the Cobot API's function GetRobotWaypoint . We then convert the values for the controller and store it in the hw_positions_ array. The ROS2 controller manager publishes this array and MoveIt2 can display the state of each joint in rviz / Gazebo. A graphical overview of the read implementation is provided in the following. write(...) \u00b6 For the arm_group (joint_0, ..., joint_6), we use the real-time singleton buffer implemented in shared_trajectory_buffer.hpp . It is filled by the cobot_trajectory_controller . Once a new buffer is received, we extract the values of the trajectory, convert these for the Cobot and call ExecuteInstructionList . For the grippers we read the commands directly ( hw_commands_[Joint::FINGER_1] ), convert them into Cobot format and call SetRobotWaypoint for each joint. The vacuum grippers are controlled independently. Moreover the API enables to can control the extension as well as the vacuum system. A graphical overview of the write implementation is provided in the following. Limitation \u00b6 Joint_0 (the prismatic joint, moving around the linear axis) is executed separately from joint_1, ... joint_6. We first execute the motion of join_0 and subsequently the trajectory of joints 1, ..., 6. This represents a limitation that is required for smooth trajectories (executing a trajectory for all joints would omit smoothing in the Cobot SPS and the motion becomes jerky). For joint_0, we also use only the last point of the trajectory => we drop all intermediate points (also due to otherwise jerky movements). This implies that trajectories for motions around obstacle which include joint_0 are not feasible on the real Cobot. We can run these only in simulation. An example of an infeasible trajectory is shown in the following: the Cobot avoids getting in contact with the pole by using joint_0. On the real Cobot, we would execute the last point of joint_0 first (resulting in no motion in joint_0 since its start position is equivalent to its end position) followed by the trajectory of joints 1, ..., 6. We believe that this limitation is acceptable for our use case as our goal is to execute pick and place tasks in uncluttered environments.","title":"Hardware Interface"},{"location":"cobot_hardware/#cobot-hardware-interface","text":"Manipulation of the Cobot with MoveIt2 and ROS2. This package encapsulates the Festo Cobot API into a ROS2 control hardware interface. The Cobot is controlled physically with this module. The package is hosted on the GitLab server of the Esslingen University repo . Hence, in order to control the Cobot, you need to be connected to the HS Esslingen VPN. Background: the package contains Festo proprietary implementations and binaries for the communication with the Cobot that were not released to the public. Therefore, this code is hosted on the Esslingen University GitLab while the remaining project is hosted on a public GitHub repository.","title":"Cobot Hardware Interface"},{"location":"cobot_hardware/#package-contents","text":"\u251c\u2500\u2500 boost_1_65_0 | \u251c\u2500\u2500 config \u2502 \u2514\u2500\u2500 festo_node_config.ini ... \u251c\u2500\u2500 lib \u2502 \u2514\u2500\u2500 libopc_ua.so | \u251c\u2500\u2500 open62541 | \u2514\u2500\u2500 src boost_1_65_0 contains an archive of the Boost version required for the build of libopc_ua.so lib contains a proprietary (by Festo) library extending the OPC-UA implementation open62541 an open source OPC-UA implementation src contains the implementation of the ROS2 controller cobot_hardware.cpp and the Cobot API robot_controller_comm.cpp","title":"Package Contents"},{"location":"cobot_hardware/#implementation-overview","text":"We have two options to pass commands to the Cobot: single point (the final point of a trajectory, via SetRobotWaypoint ) or a list of points (via ExecuteInstructionList , representing a path). We cannot send actual trajectories or fine grained position commands as the SPS of the Cobot takes care of the trajectory (and we cannot by-pass the SPS). This, unfortunately, does not align with the ROS2 control principles. A basic overview of the trajectory generation and control is provided in the following: MoveIt Planning (OMPL) \u2192 Generates a collision free path \u2193 Time Parameterization Plugin (Iterative / TOTG) \u2192 Adds time (making it an actual trajectory) \u2193 Final RobotTrajectory message \u2193 MoveIt Simple Controller Manager \u2193 ros2_control controller \u2193 HardwareInterface (this package) Now, the ROS2 controller is designed to send position commands to the hardware in real time; which is not suitable for the Cobot. In theory, we could execute SetRobotWaypoint for each position. This, however is not feasible as the resulting movement of the Cobot becomes jerky. The Cobot will interpret each point as a trajectory and will pause in between. In previous implementations, we simply sent the final trajectory point to the Cobot (using a custom MoveItControllerManager, the CobotControllerManager ). Although this resulted in smooth movements of the Cobot arm, the resulting trajectory was calculated by the SPS. We therefore had no influence on the trajectory and collisions that were considered within trajectory planning were obsolete. Hence, even with a camera attached, the Cobot would have been blind. In order to overcome this limitation we use the instruction list mechanism, where we pass a list of points to the Cobot ( ExecuteInstructionList ). The benefits of this mechanism is an underlying smoothing of the points. Details on the smoothing implementation are provided in chapter 2.19 of the Festo Robotics Manual (refer GitLab repo ). Note: the list is nothing other than a path as we do not have an influence on the timing (controlled by the SPS). Since more than just one point is now send to this hardware interface, we have implemented a custom ROS2 controller (CobotTrajectoryController), that send out either the last point of a trajectory or the entire trajectory to this interface. Note: ROS2 control implies using single joint commands (declared as double) and since a full trajectory was therefore difficult to be passed, we introduced another communication feature: a singleton realtime buffer.","title":"Implementation overview"},{"location":"cobot_hardware/#implementation-details","text":"The following paragraphs serve as a reference on the major implementation details of the hardware interface.","title":"Implementation details"},{"location":"cobot_hardware/#cmakeliststxt","text":"... a rather special one. It includes an ExternalProject : an older boost library version, that is required for libopc_ua.so . Hence, we extract the archive of the boost library, build it and link it to libopc_ua.so . This way the lib is satisfied with the older boost version and the remaining project references the system-wide boost version. ExternalProject_Add(boost165 URL file://${BOOST_ARCHIVE} CONFIGURE_COMMAND ./bootstrap.sh --with-libraries=system,filesystem --prefix=${BOOST_INSTALL_DIR} BUILD_COMMAND ./b2 link=shared install -j4 BUILD_IN_SOURCE 1 INSTALL_COMMAND ./b2 install ) #... set(Boost_INCLUDE_DIRS ${BOOST_INCLUDE_DIR}) set(Boost_LIBRARIES_165 ${BOOST_LIBRARY_DIR}/libboost_system.so ${BOOST_LIBRARY_DIR}/libboost_filesystem.so ) set_target_properties(opc-ua-api PROPERTIES IMPORTED_LOCATION \"${CMAKE_SOURCE_DIR}/lib/libopc_ua.so\" INTERFACE_INCLUDE_DIRECTORIES \"${CMAKE_SOURCE_DIR}/include\" INTERFACE_LINK_LIBRARIES \"${Boost_LIBRARIES_165}\" )","title":"CMakeLists.txt"},{"location":"cobot_hardware/#shared-trajectory-buffer","text":"Implemented as a singleton real-time buffer => accessible from different sources in real-time (enabling thread-safety). It is required to pass full trajectories from the custom ROS2 controller ( cobot_trajectory_controller ) to the hardware interface. Using this data structure, we by-pass the hardware commands for the arm_group that are filled by the standard ROS2 controllers and use the full trajectory instead.","title":"Shared Trajectory Buffer"},{"location":"cobot_hardware/#cobot_hardwarecpp","text":"Implements the following methods // initialize size of internal arrays on_init(...); // initiate communication with the Cobot on_activate(...); // send out a list of readable states for the controllers export_state_interfaces(); // send out a list of commands for the controllers export_command_interfaces(); // read states of Cobot joints read(...); // write commands to Cobot joints write(...);","title":"cobot_hardware.cpp"},{"location":"cobot_hardware/#read","text":"For each joint, we read its current state using the Cobot API's function GetRobotWaypoint . We then convert the values for the controller and store it in the hw_positions_ array. The ROS2 controller manager publishes this array and MoveIt2 can display the state of each joint in rviz / Gazebo. A graphical overview of the read implementation is provided in the following.","title":"read(...)"},{"location":"cobot_hardware/#write","text":"For the arm_group (joint_0, ..., joint_6), we use the real-time singleton buffer implemented in shared_trajectory_buffer.hpp . It is filled by the cobot_trajectory_controller . Once a new buffer is received, we extract the values of the trajectory, convert these for the Cobot and call ExecuteInstructionList . For the grippers we read the commands directly ( hw_commands_[Joint::FINGER_1] ), convert them into Cobot format and call SetRobotWaypoint for each joint. The vacuum grippers are controlled independently. Moreover the API enables to can control the extension as well as the vacuum system. A graphical overview of the write implementation is provided in the following.","title":"write(...)"},{"location":"cobot_hardware/#limitation","text":"Joint_0 (the prismatic joint, moving around the linear axis) is executed separately from joint_1, ... joint_6. We first execute the motion of join_0 and subsequently the trajectory of joints 1, ..., 6. This represents a limitation that is required for smooth trajectories (executing a trajectory for all joints would omit smoothing in the Cobot SPS and the motion becomes jerky). For joint_0, we also use only the last point of the trajectory => we drop all intermediate points (also due to otherwise jerky movements). This implies that trajectories for motions around obstacle which include joint_0 are not feasible on the real Cobot. We can run these only in simulation. An example of an infeasible trajectory is shown in the following: the Cobot avoids getting in contact with the pole by using joint_0. On the real Cobot, we would execute the last point of joint_0 first (resulting in no motion in joint_0 since its start position is equivalent to its end position) followed by the trajectory of joints 1, ..., 6. We believe that this limitation is acceptable for our use case as our goal is to execute pick and place tasks in uncluttered environments.","title":"Limitation"},{"location":"cobot_model_overview/","text":"Cobot Model Overview \u00b6 The following paragraphs serve as an overview to the basic Cobot Model characteristics. The actual specifics of the model, including our approach on the modelling of the physical attributes, is provided in the Cobot Modelling Jupyter Notebook . Robot Overview \u00b6 The cobot model defines an axis element and a corresponding mount, 7 segments (the links of the robot), 2 fingers (representing the gripper), two vacuum grippers, four TCP elements for each gripper Grippers \u00b6 We have two types of grippers: a finger gripper and a vacuum gripper system. The vacuum grippers can be controlled independently. Each gripper defines its own axis. URDF \u00b6 The URDF is implemented as a xacro to enable variation handling. We define three variations: enable_realsense_camera : to activate the real-sense camera model (deactivated by default) ros2_controls_plugin , to select the control type (fake by default) and use_collision_meshes , to select between simple and mesh based collisions (mesh by default). To generate the URDF locally run xacro src/cobot_model/urdf/festo_cobot_model.urdf.xacro -o cobot.urdf TCPs \u00b6 Since we define all rotations around the Z axis, our TCPs do not have a neutral orientation. This implies the following \"obstacle\": for a pick and place task, for example, we would position an object into the world that by default would have a neutral orientation. Since our TCP is does not have a neutral orientation, grasping the object would always imply a rotation of the TCP. Hence, for each task, we would need to transform the Cobot's TCP orientation into the object's (neutral) orientation (in the corresponding code). Since this could be tedious, we have created three additional TCP frames: one TCP aligned to the world frame, and two slightly tilted TCP frames (w.r.t. the world frame) that compensate for the Cobot's natural tilt at the tool base. Example for the gripper_tcp: gripper_tcp gripper_tcp_world gripper_tcp_world_tilted_up gripper_tcp_world_tilted_down The TCP with all rotations TCP in world frame. All rotations are inverted. TCP in world frame tilted by -45 deg. Compensates the natural tilt in the physical model, Z+ is up TCP in world frame tilted by +135 deg. Compensates the natural tilt in the physical model, Z+ is down Modelling Physical Attributes \u00b6 Neither the mass nor the inertia values were present within the original URDF. We used the following attempts to derive the mass and the inertia values. mass : we used the volume of each element and expected it to be filled with water (this resulted in an estimated mass value for each element). inertia : we approximated the elements with basic shapes (cuboid / cylinder) and applied inertia tensors using their dimension values. Result \u00b6 Visual display of inertia and mass values of the Cobot: Inertia Mass Modelling Collisions \u00b6 Collisions define boundaries of robot elements. They allow to identify physical collisions of the robot with itself or with its environment and can be either defined with simple geometric shapes or with meshes. Collision Meshes \u00b6 We used the same meshes for collisions and visuals but reduced the number of faces for the collision meshes (to ease up planning and simulation). Simple Geometric Shapes \u00b6 In order to simplify collision checking even further (for specific corner cases, for example) we have created collisions from geometric shapes. Refer use_collision_meshes parameter in xacro .","title":"Cobot Model Overview"},{"location":"cobot_model_overview/#cobot-model-overview","text":"The following paragraphs serve as an overview to the basic Cobot Model characteristics. The actual specifics of the model, including our approach on the modelling of the physical attributes, is provided in the Cobot Modelling Jupyter Notebook .","title":"Cobot Model Overview"},{"location":"cobot_model_overview/#robot-overview","text":"The cobot model defines an axis element and a corresponding mount, 7 segments (the links of the robot), 2 fingers (representing the gripper), two vacuum grippers, four TCP elements for each gripper","title":"Robot Overview"},{"location":"cobot_model_overview/#grippers","text":"We have two types of grippers: a finger gripper and a vacuum gripper system. The vacuum grippers can be controlled independently. Each gripper defines its own axis.","title":"Grippers"},{"location":"cobot_model_overview/#urdf","text":"The URDF is implemented as a xacro to enable variation handling. We define three variations: enable_realsense_camera : to activate the real-sense camera model (deactivated by default) ros2_controls_plugin , to select the control type (fake by default) and use_collision_meshes , to select between simple and mesh based collisions (mesh by default). To generate the URDF locally run xacro src/cobot_model/urdf/festo_cobot_model.urdf.xacro -o cobot.urdf","title":"URDF"},{"location":"cobot_model_overview/#tcps","text":"Since we define all rotations around the Z axis, our TCPs do not have a neutral orientation. This implies the following \"obstacle\": for a pick and place task, for example, we would position an object into the world that by default would have a neutral orientation. Since our TCP is does not have a neutral orientation, grasping the object would always imply a rotation of the TCP. Hence, for each task, we would need to transform the Cobot's TCP orientation into the object's (neutral) orientation (in the corresponding code). Since this could be tedious, we have created three additional TCP frames: one TCP aligned to the world frame, and two slightly tilted TCP frames (w.r.t. the world frame) that compensate for the Cobot's natural tilt at the tool base. Example for the gripper_tcp: gripper_tcp gripper_tcp_world gripper_tcp_world_tilted_up gripper_tcp_world_tilted_down The TCP with all rotations TCP in world frame. All rotations are inverted. TCP in world frame tilted by -45 deg. Compensates the natural tilt in the physical model, Z+ is up TCP in world frame tilted by +135 deg. Compensates the natural tilt in the physical model, Z+ is down","title":"TCPs"},{"location":"cobot_model_overview/#modelling-physical-attributes","text":"Neither the mass nor the inertia values were present within the original URDF. We used the following attempts to derive the mass and the inertia values. mass : we used the volume of each element and expected it to be filled with water (this resulted in an estimated mass value for each element). inertia : we approximated the elements with basic shapes (cuboid / cylinder) and applied inertia tensors using their dimension values.","title":"Modelling Physical Attributes"},{"location":"cobot_model_overview/#result","text":"Visual display of inertia and mass values of the Cobot: Inertia Mass","title":"Result"},{"location":"cobot_model_overview/#modelling-collisions","text":"Collisions define boundaries of robot elements. They allow to identify physical collisions of the robot with itself or with its environment and can be either defined with simple geometric shapes or with meshes.","title":"Modelling Collisions"},{"location":"cobot_model_overview/#collision-meshes","text":"We used the same meshes for collisions and visuals but reduced the number of faces for the collision meshes (to ease up planning and simulation).","title":"Collision Meshes"},{"location":"cobot_model_overview/#simple-geometric-shapes","text":"In order to simplify collision checking even further (for specific corner cases, for example) we have created collisions from geometric shapes. Refer use_collision_meshes parameter in xacro .","title":"Simple Geometric Shapes"},{"location":"cobot_trajectory_controller/","text":"Cobot Trajectory Controller \u00b6 Custom ROS2 controller for the Cobot. This controller works only in combination with the cobot_hardware package and aims to control the real Cobot. In order to manipulate the Cobot in ROS2, make sure you have pulled the cobot_hardware submodule. Run this controller with ros2 launch demo rviz_demo_launch.py controller_type:=real Implementation Overview \u00b6 We have two options to pass commands to the Cobot API: single point (the final point of a trajectory, via SetRobotWaypoint ) or a list of points (via ExecuteInstructionList , representing a path). We cannot send actual trajectories or fine grained position commands as the SPS of the Cobot takes care of the trajectory (and we cannot by-pass the SPS). This, unfortunately, does not align with the ROS2 control principles. A basic overview of the trajectory generation and control is provided in the following: MoveIt Planning (OMPL) \u2192 Generates a collision free path \u2193 Time Parameterization Plugin (Iterative / TOTG) \u2192 Adds time (making it an actual trajectory) \u2193 Final RobotTrajectory message \u2193 MoveIt Simple Controller Manager \u2193 ros2_control controller \u2193 HardwareInterface The ROS2 controllers are designed to send position commands to the hardware in real time; which is not suitable for the Cobot. In previous implementations, we simply sent the final trajectory point to the Cobot (using a custom MoveItControllerManager, the CobotControllerManager ). Although this resulted in smooth movements of the Cobot arm, the resulting trajectory was calculated by the SPS. We therefore had no influence on the trajectory and collisions that were considered within trajectory planning were obsolete. Hence, even with a camera attached, the Cobot would have been blind. In order to overcome this limitation we use the instruction list mechanism, where we pass a list of points to the Cobot and since the standard implementation of the ROS2 trajectory controller does not allow to send full trajectories (or lists of points), we are using this custom ROS2 controller, that send out either the last point of a trajectory or the entire trajectory to this interface. Note: ROS2 control implies using single joint commands (declared as double) and since a full trajectory was therefore difficult to be passed, we introduced another communication feature: a singleton realtime buffer. MoveIt Planning (OMPL) \u2192 Generates a collision free path \u2193 Time Parameterization Plugin (Iterative / TOTG) \u2192 Adds time (making it an actual trajectory) \u2193 Final RobotTrajectory message \u2193 MoveIt Simple Controller Manager \u2193 cobot_trajectory_controller (this package) \u2193 trajectory (via shared buffer) cobot_hardware Implementation Details \u00b6 The Cobot Trajectory Controller forwards trajectories to the hardware interface in three steps: accept a trajectory from the MoveItSimpeControllerManager, resample the trajectory points (optional) and place the entire trajectory or its last point into a shared buffer for the hardware interface. A communication overview between MoveIt2, the Cobot Trajectory Controller and the hardware interface is provided in the following. List of the major functions implemented: /* * Expose the command interfaces to the controller manager */ command_interface_configuration() /* * Expose the state interfaces to the controller manager */ state_interface_configuration() /* * Configure ROS2 environment */ on_configure(const rclcpp_lifecycle::State &previous_state) /* * Callback for reception of /follow_joint_trajectory */ goal_callback(...) /* * Communication with the hardware interface. * Fills a realtime buffer with a trajectory for the hardware interface. */ update(const rclcpp::Time &time, const rclcpp::Duration &period) /* * Spatial trajectory resampling for the reduction of points. */ resample_trajectory(const trajectory_msgs::msg::JointTrajectory &trajectory) /* * Initiate the service (cobot_api_service_) between this controller * and the rviz panel */ setup_service_for_rviz_panel() Graphical overview of function interactions: command_interface_configuration / state_interface_configuration \u00b6 We claim the states published by the cobot hardware interface as well as the commands send to the hardware interface. This has to be aligned with the ros2 control configuration. Refer cobot_moveit_config/ros2_controllers.yaml cobot_arm_group_controller: type: cobot_trajectory_controller/CobotTrajectoryController #... cobot_arm_group_controller: ros__parameters: joints: - joint_0 - joint_1 - joint_2 - joint_3 - joint_4 - joint_5 - joint_6 command_interfaces: - position state_interfaces: - position - velocity Note we do not access the state_interfaces in this controller; which is a rather unlikely setup. In a standard controller, we would read the states of the joints and adapt our control commands based on the current state of each joint => what a rea-time controller usually does. Since we do not have any influence on the trajectory execution (which is handled by the Cobot SPS), we simply claim the states (as defined in the configuration) but do not read them. Reading and publishing the states for the remaining system is handled byt the joint_state_broadcaster . on_configure \u00b6 Defines an action_server that listens to the /follow_joint_trajectory and executes goal_callback(...) on reception of the topic. Sets ROS2 parameters for execution_mode as well as the corresponding resampling_delta . execution_mode = single_point only the last point of a trajectory is send to the hardware interface. execution_mode = full_trajectory the full trajectory is send to the hardware interface. resampling_delta defines the spacial minimum delta between two trajectory points. This implementation enables smoother motions on the Cobot. If we send out a trajectory that has too many points, the Cobot's SPS will only be able to execute these with a jerky movement. goal_callback \u00b6 Fills an internal trajectory buffer on reception of /follow_joint_trajectory . This is required due to the asynchronous execution of update and the reception of the /follow_joint_trajectory topic. update \u00b6 The ROS2 Control Manager calls this function periodically. The implementation checks if the internal trajectory buffer was updated and runs an optional re-sampling of the received trajectory. Note we slowed down the periodic execution of this function from 100 hz (default) to 10 hz. Background: since we do not have real-time control, a high frequency execution is not required. Refer: cobot_moveit_config/ros2_controllers.yaml Resampling \u00b6 Spatial trajectory resampling for the reduction of points. In case full trajectory forwarding is active ( execution_mode == full_trajectory ), we forward the entire trajectory to the hardware interface. Since the Cobot does not accept timing based commands but a path and since dense trajectory points imply jerky movements, we resample the trajectory based on a minimum distance between two subsequent trajectory points. This can be adjusted using the parameter: resampling_delta . Example with resampling_delta = 0.5 Trajectory point: 0 1 2 3 4 5 6 7 8 -------------------------------------------------------------------------- Joint_1 pos (rad): 0.0 0.2 0.4 0.6 0.8 1.0 1.2 1.4 1.6 Selected: - - # - - # - - # Joint_2 pos (rad): 0.0 0.2 0.4 0.6 0.8 1.0 1.0 1.0 1.0 Selected: - - - - - - - - # # = selected point - = skipped point (delta < resampling_delta) Resampled trajectory point: 0 1 2 ----------------------------------------------- Resampled joint_1 pos (rad): 0.4 1.0 1.6 Resampled joint_2 pos (rad): 0.4 1.0 1.0 Note: in practice all joints move with the same velocity (we have defined the same limits for each joint in the configurations) Graphical interpretation of trajectory resampling on real data (joint 5): number of initial traj. points: 68 resampling_delta: 0.3 => number of resampled traj. points: 5 CobotApiSrv \u00b6 execution_mode and resampling_delta are defined as parameters that can be updated on demand using: ros2 param set /cobot_arm_group_controller execution_mode full_trajectory ros2 param set /cobot_arm_group_controller resampling_delta 0.3 In order to increase usability, we have implemented a small rviz plugin that communicates with the Cobot Trajectory Controller and that enables updating these parameters in rviz.","title":"Trajectory Controller"},{"location":"cobot_trajectory_controller/#cobot-trajectory-controller","text":"Custom ROS2 controller for the Cobot. This controller works only in combination with the cobot_hardware package and aims to control the real Cobot. In order to manipulate the Cobot in ROS2, make sure you have pulled the cobot_hardware submodule. Run this controller with ros2 launch demo rviz_demo_launch.py controller_type:=real","title":"Cobot Trajectory Controller"},{"location":"cobot_trajectory_controller/#implementation-overview","text":"We have two options to pass commands to the Cobot API: single point (the final point of a trajectory, via SetRobotWaypoint ) or a list of points (via ExecuteInstructionList , representing a path). We cannot send actual trajectories or fine grained position commands as the SPS of the Cobot takes care of the trajectory (and we cannot by-pass the SPS). This, unfortunately, does not align with the ROS2 control principles. A basic overview of the trajectory generation and control is provided in the following: MoveIt Planning (OMPL) \u2192 Generates a collision free path \u2193 Time Parameterization Plugin (Iterative / TOTG) \u2192 Adds time (making it an actual trajectory) \u2193 Final RobotTrajectory message \u2193 MoveIt Simple Controller Manager \u2193 ros2_control controller \u2193 HardwareInterface The ROS2 controllers are designed to send position commands to the hardware in real time; which is not suitable for the Cobot. In previous implementations, we simply sent the final trajectory point to the Cobot (using a custom MoveItControllerManager, the CobotControllerManager ). Although this resulted in smooth movements of the Cobot arm, the resulting trajectory was calculated by the SPS. We therefore had no influence on the trajectory and collisions that were considered within trajectory planning were obsolete. Hence, even with a camera attached, the Cobot would have been blind. In order to overcome this limitation we use the instruction list mechanism, where we pass a list of points to the Cobot and since the standard implementation of the ROS2 trajectory controller does not allow to send full trajectories (or lists of points), we are using this custom ROS2 controller, that send out either the last point of a trajectory or the entire trajectory to this interface. Note: ROS2 control implies using single joint commands (declared as double) and since a full trajectory was therefore difficult to be passed, we introduced another communication feature: a singleton realtime buffer. MoveIt Planning (OMPL) \u2192 Generates a collision free path \u2193 Time Parameterization Plugin (Iterative / TOTG) \u2192 Adds time (making it an actual trajectory) \u2193 Final RobotTrajectory message \u2193 MoveIt Simple Controller Manager \u2193 cobot_trajectory_controller (this package) \u2193 trajectory (via shared buffer) cobot_hardware","title":"Implementation Overview"},{"location":"cobot_trajectory_controller/#implementation-details","text":"The Cobot Trajectory Controller forwards trajectories to the hardware interface in three steps: accept a trajectory from the MoveItSimpeControllerManager, resample the trajectory points (optional) and place the entire trajectory or its last point into a shared buffer for the hardware interface. A communication overview between MoveIt2, the Cobot Trajectory Controller and the hardware interface is provided in the following. List of the major functions implemented: /* * Expose the command interfaces to the controller manager */ command_interface_configuration() /* * Expose the state interfaces to the controller manager */ state_interface_configuration() /* * Configure ROS2 environment */ on_configure(const rclcpp_lifecycle::State &previous_state) /* * Callback for reception of /follow_joint_trajectory */ goal_callback(...) /* * Communication with the hardware interface. * Fills a realtime buffer with a trajectory for the hardware interface. */ update(const rclcpp::Time &time, const rclcpp::Duration &period) /* * Spatial trajectory resampling for the reduction of points. */ resample_trajectory(const trajectory_msgs::msg::JointTrajectory &trajectory) /* * Initiate the service (cobot_api_service_) between this controller * and the rviz panel */ setup_service_for_rviz_panel() Graphical overview of function interactions:","title":"Implementation Details"},{"location":"cobot_trajectory_controller/#command_interface_configuration-state_interface_configuration","text":"We claim the states published by the cobot hardware interface as well as the commands send to the hardware interface. This has to be aligned with the ros2 control configuration. Refer cobot_moveit_config/ros2_controllers.yaml cobot_arm_group_controller: type: cobot_trajectory_controller/CobotTrajectoryController #... cobot_arm_group_controller: ros__parameters: joints: - joint_0 - joint_1 - joint_2 - joint_3 - joint_4 - joint_5 - joint_6 command_interfaces: - position state_interfaces: - position - velocity Note we do not access the state_interfaces in this controller; which is a rather unlikely setup. In a standard controller, we would read the states of the joints and adapt our control commands based on the current state of each joint => what a rea-time controller usually does. Since we do not have any influence on the trajectory execution (which is handled by the Cobot SPS), we simply claim the states (as defined in the configuration) but do not read them. Reading and publishing the states for the remaining system is handled byt the joint_state_broadcaster .","title":"command_interface_configuration / state_interface_configuration"},{"location":"cobot_trajectory_controller/#on_configure","text":"Defines an action_server that listens to the /follow_joint_trajectory and executes goal_callback(...) on reception of the topic. Sets ROS2 parameters for execution_mode as well as the corresponding resampling_delta . execution_mode = single_point only the last point of a trajectory is send to the hardware interface. execution_mode = full_trajectory the full trajectory is send to the hardware interface. resampling_delta defines the spacial minimum delta between two trajectory points. This implementation enables smoother motions on the Cobot. If we send out a trajectory that has too many points, the Cobot's SPS will only be able to execute these with a jerky movement.","title":"on_configure"},{"location":"cobot_trajectory_controller/#goal_callback","text":"Fills an internal trajectory buffer on reception of /follow_joint_trajectory . This is required due to the asynchronous execution of update and the reception of the /follow_joint_trajectory topic.","title":"goal_callback"},{"location":"cobot_trajectory_controller/#update","text":"The ROS2 Control Manager calls this function periodically. The implementation checks if the internal trajectory buffer was updated and runs an optional re-sampling of the received trajectory. Note we slowed down the periodic execution of this function from 100 hz (default) to 10 hz. Background: since we do not have real-time control, a high frequency execution is not required. Refer: cobot_moveit_config/ros2_controllers.yaml","title":"update"},{"location":"cobot_trajectory_controller/#resampling","text":"Spatial trajectory resampling for the reduction of points. In case full trajectory forwarding is active ( execution_mode == full_trajectory ), we forward the entire trajectory to the hardware interface. Since the Cobot does not accept timing based commands but a path and since dense trajectory points imply jerky movements, we resample the trajectory based on a minimum distance between two subsequent trajectory points. This can be adjusted using the parameter: resampling_delta . Example with resampling_delta = 0.5 Trajectory point: 0 1 2 3 4 5 6 7 8 -------------------------------------------------------------------------- Joint_1 pos (rad): 0.0 0.2 0.4 0.6 0.8 1.0 1.2 1.4 1.6 Selected: - - # - - # - - # Joint_2 pos (rad): 0.0 0.2 0.4 0.6 0.8 1.0 1.0 1.0 1.0 Selected: - - - - - - - - # # = selected point - = skipped point (delta < resampling_delta) Resampled trajectory point: 0 1 2 ----------------------------------------------- Resampled joint_1 pos (rad): 0.4 1.0 1.6 Resampled joint_2 pos (rad): 0.4 1.0 1.0 Note: in practice all joints move with the same velocity (we have defined the same limits for each joint in the configurations) Graphical interpretation of trajectory resampling on real data (joint 5): number of initial traj. points: 68 resampling_delta: 0.3 => number of resampled traj. points: 5","title":"Resampling"},{"location":"cobot_trajectory_controller/#cobotapisrv","text":"execution_mode and resampling_delta are defined as parameters that can be updated on demand using: ros2 param set /cobot_arm_group_controller execution_mode full_trajectory ros2 param set /cobot_arm_group_controller resampling_delta 0.3 In order to increase usability, we have implemented a small rviz plugin that communicates with the Cobot Trajectory Controller and that enables updating these parameters in rviz.","title":"CobotApiSrv"},{"location":"controller_manager/","text":"Cobot Controller Manager \u00b6 Discontinued \u00b6 This solution is currently not active in the Cobot project. It has been replaced by a custom trajectory controller. To reactivate this solution, revert commit: 9a403d50ed45b9cc583237b94f2c068a97f4bd74 and use the following branch for the cobot_hardware : adapt-hw-interface-to-custom-controller-manager . Reason \u00b6 Although this solution results in smooth movements of the Cobot arm, the resulting trajectory is calculated by the SPS of the Cobot. We therefore have no influence on the trajectory and collisions that were considered within trajectory planning are obsolete. Hence, even with a camera attached, the Cobot will be blind with this solution. Implementation Overview \u00b6 Our Cobot is not able to follow a full trajectory. It instead accepts the final trajectory point or a list of points (in form of a path). MoveIt2 and ROS2 control, however are designed to generate a trajectory and send out single position / velocity / effort commands to the hardware in realtime. Since we cannot use these single points, we use the last trajectory point and forward it to the Cobot hardware interface. Communication Flow \u00b6 cobot_controller_manager (this package) \u2193 (send last trajectory point) ros2_control controller \u2193 (forward one single point to hardware interface) cobot_hardware (real hardware interface) Getting started \u00b6 This controller manager works only in combination with the cobot_hardware package (branch: adapt-hw-interface-to-custom-controller-manager ) and aims to control the real Cobot. In order to manipulate the Cobot in ROS2, connect to HS-Esslingen VPN and run git clone https://github.com/robgineer/cobot.git . git submodule init src/cobot_hardware git submodule update src/cobot_hardware Then build this project and run ros2 launch demo rviz_demo_launch.py controller_type:=real","title":"Controller Manager"},{"location":"controller_manager/#cobot-controller-manager","text":"","title":"Cobot Controller Manager"},{"location":"controller_manager/#discontinued","text":"This solution is currently not active in the Cobot project. It has been replaced by a custom trajectory controller. To reactivate this solution, revert commit: 9a403d50ed45b9cc583237b94f2c068a97f4bd74 and use the following branch for the cobot_hardware : adapt-hw-interface-to-custom-controller-manager .","title":"Discontinued"},{"location":"controller_manager/#reason","text":"Although this solution results in smooth movements of the Cobot arm, the resulting trajectory is calculated by the SPS of the Cobot. We therefore have no influence on the trajectory and collisions that were considered within trajectory planning are obsolete. Hence, even with a camera attached, the Cobot will be blind with this solution.","title":"Reason"},{"location":"controller_manager/#implementation-overview","text":"Our Cobot is not able to follow a full trajectory. It instead accepts the final trajectory point or a list of points (in form of a path). MoveIt2 and ROS2 control, however are designed to generate a trajectory and send out single position / velocity / effort commands to the hardware in realtime. Since we cannot use these single points, we use the last trajectory point and forward it to the Cobot hardware interface.","title":"Implementation Overview"},{"location":"controller_manager/#communication-flow","text":"cobot_controller_manager (this package) \u2193 (send last trajectory point) ros2_control controller \u2193 (forward one single point to hardware interface) cobot_hardware (real hardware interface)","title":"Communication Flow"},{"location":"controller_manager/#getting-started","text":"This controller manager works only in combination with the cobot_hardware package (branch: adapt-hw-interface-to-custom-controller-manager ) and aims to control the real Cobot. In order to manipulate the Cobot in ROS2, connect to HS-Esslingen VPN and run git clone https://github.com/robgineer/cobot.git . git submodule init src/cobot_hardware git submodule update src/cobot_hardware Then build this project and run ros2 launch demo rviz_demo_launch.py controller_type:=real","title":"Getting started"},{"location":"gui_overview/","text":"VNC vs Xpra \u00b6 We offer two different options for displaying the graphical user interface: xpra or VNC. Both are different and have their specific use cases. The VNC option enables viewing an entire Ubuntu Desktop within your browser. Useful in case you are running this project on a local machine. Refer VNC setup for details and configuration. The xpra option enables forwarding single X11 windows from your terminal. Its a bit more lightweight and useful if you are running this project on a remote machine. It requires your user to be ported into the docker container, however. This might not work on all configurations. Refer Xpra setup for details and configuration.","title":"VNC vs Xpra"},{"location":"gui_overview/#vnc-vs-xpra","text":"We offer two different options for displaying the graphical user interface: xpra or VNC. Both are different and have their specific use cases. The VNC option enables viewing an entire Ubuntu Desktop within your browser. Useful in case you are running this project on a local machine. Refer VNC setup for details and configuration. The xpra option enables forwarding single X11 windows from your terminal. Its a bit more lightweight and useful if you are running this project on a remote machine. It requires your user to be ported into the docker container, however. This might not work on all configurations. Refer Xpra setup for details and configuration.","title":"VNC vs Xpra"},{"location":"howToVNC/","text":"VNC \u00b6 This folder allows opening the cobot docker in a vscode devcontainer and display graphics in your browser via VNC. Preparation \u00b6 You need the following installed: docker (as described in the Quickstart guide ) Visual Studio Code Install the Dev Containers Extension in Visual Studio Code. Install Devcontainer \u00b6 Clone the cobot repo: git clone https://github.com/robgineer/cobot.git . cd cobot and open the cloned cobot directory in Visual Studio Code (File - Open Folder - [select the cobot directory]). Press the \"F1\" key and type \"dev containers: rebuild and reopen container\". Run this command. This will download and build the docker container (might take a few minutes). Once this is done, you can open a new terminal in VS Code which should look something like this: To run a command as administrator (user \"root\"), use \"sudo <command>\". See \"man sudo_root\" for details. ubuntu@ros2-vnc-docker:/workspace$ ls LICENSE README.md scripts src Your user is \"ubuntu\" and password is also \"ubuntu\". On your host machine, open http://127.0.0.1:6080 in a browser. This should open a noVNC screen, and if you click connect, you should be able to use an XFCE Desktop of your docker container. You may also open the ubuntu desktop within Visual Studio Code. Just open the \"Ports\" tab and use the preview icon: As a test, open a terminal in VS Code and start xeyes on the Ubuntu Desktop: ubuntu@ros2-vnc-docker:/workspace$ export DISPLAY=:1 ubuntu@ros2-vnc-docker:/workspace$ xeyes Build and run the project \u00b6 Finally, lets build the project: ubuntu@ros2-vnc-docker:~$ cd /workspace ubuntu@ros2-vnc-docker:/workspace$ colcon build --merge-install --symlink-install --cmake-args \"-DCMAKE_BUILD_TYPE=Release\" ubuntu@ros2-vnc-docker:/workspace$ source install/setup.bash Switch to the browser window http://127.0.0.1:6080 and start the demo in a new terminal: ubuntu@ros2-vnc-docker:~$ cd /workspace ubuntu@ros2-vnc-docker:/workspace$ source install/setup.bash ubuntu@ros2-vnc-docker:/workspace$ ros2 launch cobot_moveit_config gz_demo_launch.py This should look similar to this: Done! Running on a remote server \u00b6 You can also run the container on a remote machine via ssh. In order to see the VNC desktop on your local machine, you need to set up an ssh tunnel on your local machine: ssh -N -f -L 6081:localhost:6080 <your-user-id>@<remote-machine> Now start vscode on your local machine, connect to your remote server, and open the project in a devcontainer. You can than view the VNC desktop on your local browser at http://127.0.0.1:6081 .","title":"VNC"},{"location":"howToVNC/#vnc","text":"This folder allows opening the cobot docker in a vscode devcontainer and display graphics in your browser via VNC.","title":"VNC"},{"location":"howToVNC/#preparation","text":"You need the following installed: docker (as described in the Quickstart guide ) Visual Studio Code Install the Dev Containers Extension in Visual Studio Code.","title":"Preparation"},{"location":"howToVNC/#install-devcontainer","text":"Clone the cobot repo: git clone https://github.com/robgineer/cobot.git . cd cobot and open the cloned cobot directory in Visual Studio Code (File - Open Folder - [select the cobot directory]). Press the \"F1\" key and type \"dev containers: rebuild and reopen container\". Run this command. This will download and build the docker container (might take a few minutes). Once this is done, you can open a new terminal in VS Code which should look something like this: To run a command as administrator (user \"root\"), use \"sudo <command>\". See \"man sudo_root\" for details. ubuntu@ros2-vnc-docker:/workspace$ ls LICENSE README.md scripts src Your user is \"ubuntu\" and password is also \"ubuntu\". On your host machine, open http://127.0.0.1:6080 in a browser. This should open a noVNC screen, and if you click connect, you should be able to use an XFCE Desktop of your docker container. You may also open the ubuntu desktop within Visual Studio Code. Just open the \"Ports\" tab and use the preview icon: As a test, open a terminal in VS Code and start xeyes on the Ubuntu Desktop: ubuntu@ros2-vnc-docker:/workspace$ export DISPLAY=:1 ubuntu@ros2-vnc-docker:/workspace$ xeyes","title":"Install Devcontainer"},{"location":"howToVNC/#build-and-run-the-project","text":"Finally, lets build the project: ubuntu@ros2-vnc-docker:~$ cd /workspace ubuntu@ros2-vnc-docker:/workspace$ colcon build --merge-install --symlink-install --cmake-args \"-DCMAKE_BUILD_TYPE=Release\" ubuntu@ros2-vnc-docker:/workspace$ source install/setup.bash Switch to the browser window http://127.0.0.1:6080 and start the demo in a new terminal: ubuntu@ros2-vnc-docker:~$ cd /workspace ubuntu@ros2-vnc-docker:/workspace$ source install/setup.bash ubuntu@ros2-vnc-docker:/workspace$ ros2 launch cobot_moveit_config gz_demo_launch.py This should look similar to this: Done!","title":"Build and run the project"},{"location":"howToVNC/#running-on-a-remote-server","text":"You can also run the container on a remote machine via ssh. In order to see the VNC desktop on your local machine, you need to set up an ssh tunnel on your local machine: ssh -N -f -L 6081:localhost:6080 <your-user-id>@<remote-machine> Now start vscode on your local machine, connect to your remote server, and open the project in a devcontainer. You can than view the VNC desktop on your local browser at http://127.0.0.1:6081 .","title":"Running on a remote server"},{"location":"howToXpra/","text":"Xpra \u00b6 The docker container will enable X11 forwarding to client machines using xpra. This is particularly useful if the container runs on a remote server. Note: although SSH access and X11 forwarding within docker container is (sometimes) referred to as an \"anti-pattern\" (apparently this implies treating the docker container like a VM), we believe that this solution is the way-to-go for containers hosted on remote servers that contains all required dependencies for the development of this project. 1. Install xpra on host and client system \u00b6 In order for xpra to forward X11 sessions from the container, you need to have xpra installed on your host system (where your docker container will be running). If your host system is Ubuntu, simply use the following code snippet to download and install xpra. If not, download it from https://github.com/Xpra-org/xpra/ . sudo wget -O \"/usr/share/keyrings/xpra.asc\" https://xpra.org/xpra.asc && \\ sudo wget -P /etc/apt/sources.list.d/ https://raw.githubusercontent.com/Xpra-org/xpra/master/packaging/repos/$(lsb_release -sc)/xpra.sources && \\ sudo apt-get update && \\ sudo apt-get install -y xpra Dont forget to install xpra on your local client machine as well. 2. Build the docker image \u00b6 After successfully having cloned this repo, run the docker_configuration script. cd ~/cobot/scripts ./docker_configuration.sh The configuration script contains the following steps: 1. Builds the docker container and creates a user with the same username, user id and the group id as the user executing the script. This means your current user will be present in the docker container. 2. Mounts your home directory, exposes /etc/shadows (for authentication), /run/dbus/system_bus_socket (for xpra communication) and the usb ports. The container within the same network as the host and in --priviledged mode (required for USB access). Porting the current user to the docker container is unfortunately essential to enable a seamless xpra integration. Using this setup, you will be able to authenticate in your xpra session with the same user and password as on your host. Troubleshooting \u00b6 If building the docker container fails due to the following error: 0.276 groupadd: GID '1000' already exists then you are probably running on a host that has only one user (your user) and the user id is the same as the ubuntu user (uid=1000) created within the docker build. Unfortunately we could not find elegant solutions for this issue. Some options are creating a different user ( uid!=1000 ) and re-running the docker_configuration script or deleting the ubuntu user within the dockerfile. As mentioned above, it is absolutely necessary to port your current user into the docker container (otherwise xpra authentications will fail). If you cannot create a new user on your machine, kindly check out the VNC option . 3. Run X11 sessions from your terminal \u00b6 In the docker container run: export DISPLAY=:100 # or any display number you prefer that is not used xpra start :100 On client run: xpra attach ssh://<user>@<server>:22/100 Where \\<server> represents the address of your remote host that runs the docker container. X11 will be forwarded from the docker container via the host to your client. All graphical user interfaces started from the terminal of the docker container will be forwarded to the client machine. You can run multiple xpra sessions from the same container but different terminals. Simply change the DISPLAY number for different sessions.","title":"Xpra"},{"location":"howToXpra/#xpra","text":"The docker container will enable X11 forwarding to client machines using xpra. This is particularly useful if the container runs on a remote server. Note: although SSH access and X11 forwarding within docker container is (sometimes) referred to as an \"anti-pattern\" (apparently this implies treating the docker container like a VM), we believe that this solution is the way-to-go for containers hosted on remote servers that contains all required dependencies for the development of this project.","title":"Xpra"},{"location":"howToXpra/#1-install-xpra-on-host-and-client-system","text":"In order for xpra to forward X11 sessions from the container, you need to have xpra installed on your host system (where your docker container will be running). If your host system is Ubuntu, simply use the following code snippet to download and install xpra. If not, download it from https://github.com/Xpra-org/xpra/ . sudo wget -O \"/usr/share/keyrings/xpra.asc\" https://xpra.org/xpra.asc && \\ sudo wget -P /etc/apt/sources.list.d/ https://raw.githubusercontent.com/Xpra-org/xpra/master/packaging/repos/$(lsb_release -sc)/xpra.sources && \\ sudo apt-get update && \\ sudo apt-get install -y xpra Dont forget to install xpra on your local client machine as well.","title":"1. Install xpra on host and client system"},{"location":"howToXpra/#2-build-the-docker-image","text":"After successfully having cloned this repo, run the docker_configuration script. cd ~/cobot/scripts ./docker_configuration.sh The configuration script contains the following steps: 1. Builds the docker container and creates a user with the same username, user id and the group id as the user executing the script. This means your current user will be present in the docker container. 2. Mounts your home directory, exposes /etc/shadows (for authentication), /run/dbus/system_bus_socket (for xpra communication) and the usb ports. The container within the same network as the host and in --priviledged mode (required for USB access). Porting the current user to the docker container is unfortunately essential to enable a seamless xpra integration. Using this setup, you will be able to authenticate in your xpra session with the same user and password as on your host.","title":"2. Build the docker image"},{"location":"howToXpra/#troubleshooting","text":"If building the docker container fails due to the following error: 0.276 groupadd: GID '1000' already exists then you are probably running on a host that has only one user (your user) and the user id is the same as the ubuntu user (uid=1000) created within the docker build. Unfortunately we could not find elegant solutions for this issue. Some options are creating a different user ( uid!=1000 ) and re-running the docker_configuration script or deleting the ubuntu user within the dockerfile. As mentioned above, it is absolutely necessary to port your current user into the docker container (otherwise xpra authentications will fail). If you cannot create a new user on your machine, kindly check out the VNC option .","title":"Troubleshooting"},{"location":"howToXpra/#3-run-x11-sessions-from-your-terminal","text":"In the docker container run: export DISPLAY=:100 # or any display number you prefer that is not used xpra start :100 On client run: xpra attach ssh://<user>@<server>:22/100 Where \\<server> represents the address of your remote host that runs the docker container. X11 will be forwarded from the docker container via the host to your client. All graphical user interfaces started from the terminal of the docker container will be forwarded to the client machine. You can run multiple xpra sessions from the same container but different terminals. Simply change the DISPLAY number for different sessions.","title":"3. Run X11 sessions from your terminal"},{"location":"launch_files/","text":"Launch Files Explained \u00b6 Since launch files can be a bit tricky to implement and understand, the following paragraphs focus on their basics. The project contains two major launch files: src/cobot_moveit_config/launch/gz_demo_launch.py src/demo/launch/rviz_demo_launch.py Both have the same basic structure; which is explained in the following. 1. Load the Configuration Files \u00b6 We need to load and parse the configuration for the robot model ( festo_cobot_model.urdf.xacro ), robot sematics ( festo_cobot_model.srdf ), IK config ( kinematics.yaml ), joint limits ( joint_limits.yaml ) and the moveit / ros2_control configuration ( moveit_controllers.yaml / ros2_controllers.yaml ). We additionally define parameters that are not included in the yaml files (for the planning scene monitor and the ompl planner pipeline). All of these are default parameters and have been taken over from other ros2_control project examples. 2. Start the ROS2 Nodes \u00b6 We start the following nodes: ros2_control controllers (defined in ros2_controllers.yaml ), the joint_state_broadcaster to broadcast the current joint states, the robot state publisher (that publishes the URDF as well as the joint states from the joint_state_broadcaster ), rviz and the move_group (the MoveIt2 node). The move_group communicates with the user interfaces (C++ API / Python API / rviz) as well as with the sensors and the ROS2 controllers. An overview of the move_group node is provided in the following: Source: https://moveit.picknik.ai For gazebo ( src/cobot_moveit_config/launch/gz_demo_launch.py ) we also start the gazebo node and in the demo ( src/demo/launch/rviz_demo_launch.py ) we start the real-sense camera node. Note that we define several parameters in src/demo/launch/rviz_demo_launch.py : enable_realsense_camera to start real sense camera; false by default. use_collision_meshes select to either either use actual collision meshes ( true ) or simple geometric objects ( false ); true by default. controller_type using rviz only ( fake ) or the Cobot Trajectory Controller ( real ); fake by default. (Opaque function: we need to know the parameters during execution for the xacro .) The additional launch file for the pick and place example ( src/demo/launch/pick_place_launch.py ) is required only due to the fact that the moveit_task_constructor_demo expects several launch arguments (such as the URDF model and the SRDF configuration).","title":"Launch Files Explained"},{"location":"launch_files/#launch-files-explained","text":"Since launch files can be a bit tricky to implement and understand, the following paragraphs focus on their basics. The project contains two major launch files: src/cobot_moveit_config/launch/gz_demo_launch.py src/demo/launch/rviz_demo_launch.py Both have the same basic structure; which is explained in the following.","title":"Launch Files Explained"},{"location":"launch_files/#1-load-the-configuration-files","text":"We need to load and parse the configuration for the robot model ( festo_cobot_model.urdf.xacro ), robot sematics ( festo_cobot_model.srdf ), IK config ( kinematics.yaml ), joint limits ( joint_limits.yaml ) and the moveit / ros2_control configuration ( moveit_controllers.yaml / ros2_controllers.yaml ). We additionally define parameters that are not included in the yaml files (for the planning scene monitor and the ompl planner pipeline). All of these are default parameters and have been taken over from other ros2_control project examples.","title":"1. Load the Configuration Files"},{"location":"launch_files/#2-start-the-ros2-nodes","text":"We start the following nodes: ros2_control controllers (defined in ros2_controllers.yaml ), the joint_state_broadcaster to broadcast the current joint states, the robot state publisher (that publishes the URDF as well as the joint states from the joint_state_broadcaster ), rviz and the move_group (the MoveIt2 node). The move_group communicates with the user interfaces (C++ API / Python API / rviz) as well as with the sensors and the ROS2 controllers. An overview of the move_group node is provided in the following: Source: https://moveit.picknik.ai For gazebo ( src/cobot_moveit_config/launch/gz_demo_launch.py ) we also start the gazebo node and in the demo ( src/demo/launch/rviz_demo_launch.py ) we start the real-sense camera node. Note that we define several parameters in src/demo/launch/rviz_demo_launch.py : enable_realsense_camera to start real sense camera; false by default. use_collision_meshes select to either either use actual collision meshes ( true ) or simple geometric objects ( false ); true by default. controller_type using rviz only ( fake ) or the Cobot Trajectory Controller ( real ); fake by default. (Opaque function: we need to know the parameters during execution for the xacro .) The additional launch file for the pick and place example ( src/demo/launch/pick_place_launch.py ) is required only due to the fact that the moveit_task_constructor_demo expects several launch arguments (such as the URDF model and the SRDF configuration).","title":"2. Start the ROS2 Nodes"},{"location":"offline_hand_eye/","text":"Offline Hand-Eye Calibration \u00b6 This is a package for hand-eye calibration of a manipulator arm with a static camera. It has been tested with ROS2 jazzy on our robot at the University of Applied Sciences in Esslingen. Although there are very nice packages for hand-eye calibration available (notably easy_handeye2, see link below), we have decided to compile a new package for two reasons: Several times in the past, we have encountered difficulties due to incompatibilities with different ROS versions and libraries such as OpenCV - what is working now may not work anymore in the next major ROS version. Thus, we want a solution that is rather independent of ROS. We want some introspection of the calibration process, e.g. inspect individual data frames and potentially remove them from the calibration process or other manual debugging. A python version is well suited for this. The calibration process consists of three major steps (detailed further below): Extract frames : obtain camera information, camera images and robot coordinate transformations (using tf2_ros) for selected calibration frames. Calibrate : Detect markers in the image (using apriltags) and recover the extrinsic parameters of the camera (using OpenCV). Publish results : Publish the calibration results in our ros2 environment. In this process, only the first and last step should include ROS to keep dependencies at bare minimum. The package is a compilation and adaptation of other work. Here are some important links: https://github.com/marcoesposito1988/easy_handeye2 https://github.com/AprilRobotics/apriltag (with Python bindings) ROS2 TF preliminaries \u00b6 Running the calibration requires awareness of existing coordinate frames in our environment. This is done via tf2_ros . To get an overview of the tree of coordinate transformations in your environment, you can create a pdf of all frames with ros2 run tf2_tools view_frames And to check the current transformations between two frames via command line, use #Usage: tf2_echo source_frame target_frame ros2 run tf2_ros tf2_echo base_link camera_color_frame ros2 run tf2_ros tf2_echo base_link camera_color_optical_frame In our case, the excerpt of our camera transformations (i.e. the objectives of our calibration) are structured as follows: The coordinate frames of the Realsense camera in ROS are described here: realsense-ros . Please note that in our setup, the root of the camera node (which also defines its global pose) is the TF camera_bottom_screw_frame . The perspective projection of the camera is computed wrt camera_color_optical_frame . This may be different for your setup, of course. Extract frames \u00b6 For calibration, we rigidly attach an Apriltag to our robot's endeffector and record sample images and corresponding TFs. The tag can be printed from this pdf: img/tag36h11_0_140mm.pdf or generated with this online generator . Make sure to print with actual size and note the size of the marker (for the tag in this repo, it is 0.14 meters). Next, start the robot and camera ros2 launch demo rviz_demo_launch.py controller_type:=real enable_realsense_camera:=true and record sample frames: ros2 run offline_hand_eye record_calib_data This will open the following GUI where parameters have been adapted from easy_handeye2 : Calibration Type : eye_on_base for a static camera, eye_in_hand when camera is attached to the robot arm Camera Image Topic : ROS topic with the color image of your camera Camera Info Topic : ROS topic with intrinsic parameters of your camera Robot Base Frame : the base or world frame of your environment Robot Effector Frame : the TF frame of the robot's end effector Tracking Base Frame : the TF frame of the camera, markers are reconstructed wrt to this Tracking Marker Frame : optional, the TF of detected markers if these markers are detected online while extracting. Recording Directory : all extracted frames will be stored here in pickle format. Once you have set or loaded these parametes, you can start to move the robot arm around and record some frames. Recording can be done continuously (not recommended at the moment, see issue below) or in single shots. Rule of thumb is to record 12 frames or more. Make sure that the calibration target is moved across a large part of the camera image, shows variation in distance to the camera and significant variations in orientation, as e.g. here: Make sure to save your configuration parameters! Default location for this file is in the root of your repo with filename handeye_calibration_params.json . You may also use a recording and run record_calib_data from the replay (but make sure that the TFs are all received properly): ros2 bag record --all ros2 bag play <your_recording> Timing Issue : When querying TFs for the time stamp of the current image, we often do not receive valid results in our current environment. As a workaround for this problem, we acquire frames only after the robot is standing completely still, i.e. move the robot to a new position, wait shortly and make sure that rviz shows the correct setup, record sample frame, repeat. That way, we can safely utilize the latest TFs, cf. record_calib_data.py . This should be fixed! Calibrate \u00b6 The calibration process itself does not need ROS! So we can focus on the actual computation (almost any version of opencv and numpy) and debugging. To install a virtual environment and a jupyter kernel, use: conda create --name offline-hand-eye python=3.12 ipython jupyter conda-forge::matplotlib conda activate offline-hand-eye pip3 install opencv-python conda install conda-forge::apriltag python -m ipykernel install --user --name=offline-hand-eye This has been tested with miniconda, but should also work with venv. A good starting point for understanding the calibration process is the Jupyter notebook in . A more comfortable version of the calibration is using a GUI where you can check, select, and exclude different frames for calibration: conda activate offline-hand-eye cd scripts ./offline_hand_eye_calibration_gui.py --data_path ../doc/sample_data/calibration/calibdata_2025_08_11-11_51_22 --config ../../../handeye_calibration_params.json --output ../../../handeye_calibration.json Publish results \u00b6 After the calibration has been computed, you can publish the resulting TFs. Start the environment: ros2 launch demo rviz_demo_launch.py controller_type:=real enable_realsense_camera:=true and publish the calibrated TFs: ros2 run offline_hand_eye calib_publisher --ros-args -p calibration_file:=handeye_calibration.json Now, run ros2 run tf2_ros tf2_echo base_link camera_bottom_screw_frame to print the extrinsic calibration in the console, e.g. At time 0.0 - Translation: [0.659, 0.459, 1.550] - Rotation: in Quaternion [-0.604, 0.696, 0.293, 0.254] - Rotation: in RPY (radian) [2.999, 0.787, -1.771] - Rotation: in RPY (degree) [171.830, 45.092, -101.471] - Matrix: -0.140 -0.990 0.000 0.659 -0.692 0.098 0.715 0.459 -0.708 0.100 -0.699 1.550 0.000 0.000 0.000 1.000 You may also use this translation and rotation in RPY (radian) to adjust the camera transform in . For future use: running tag detectors online \u00b6 To run the apriltag detector online, use: sudo apt install ros-jazzy-apriltag-ros ros2 run apriltag_ros apriltag_node --ros-args \\ -r image_rect:=/camera/camera/color/image_raw \\ -r camera_info:=/camera/camera/color/camera_info \\ --params-file /workspace/src/offline_hand_eye/doc/tags_36h11.yaml Or for the aruco marker: sudo apt install ros-jazzy-aruco-ros ros2 run aruco_ros single --ros-args -p image_is_rectified:=true -p marker_size:=0.1 -p marker_id:=1 -p reference_frame:=camera_link -p camera_frame:=/camera/camera/color/image_raw -p marker_frame:=camera_marker -p corner_refinement:=LINES","title":"Hand-eye-calibration"},{"location":"offline_hand_eye/#offline-hand-eye-calibration","text":"This is a package for hand-eye calibration of a manipulator arm with a static camera. It has been tested with ROS2 jazzy on our robot at the University of Applied Sciences in Esslingen. Although there are very nice packages for hand-eye calibration available (notably easy_handeye2, see link below), we have decided to compile a new package for two reasons: Several times in the past, we have encountered difficulties due to incompatibilities with different ROS versions and libraries such as OpenCV - what is working now may not work anymore in the next major ROS version. Thus, we want a solution that is rather independent of ROS. We want some introspection of the calibration process, e.g. inspect individual data frames and potentially remove them from the calibration process or other manual debugging. A python version is well suited for this. The calibration process consists of three major steps (detailed further below): Extract frames : obtain camera information, camera images and robot coordinate transformations (using tf2_ros) for selected calibration frames. Calibrate : Detect markers in the image (using apriltags) and recover the extrinsic parameters of the camera (using OpenCV). Publish results : Publish the calibration results in our ros2 environment. In this process, only the first and last step should include ROS to keep dependencies at bare minimum. The package is a compilation and adaptation of other work. Here are some important links: https://github.com/marcoesposito1988/easy_handeye2 https://github.com/AprilRobotics/apriltag (with Python bindings)","title":"Offline Hand-Eye Calibration"},{"location":"offline_hand_eye/#ros2-tf-preliminaries","text":"Running the calibration requires awareness of existing coordinate frames in our environment. This is done via tf2_ros . To get an overview of the tree of coordinate transformations in your environment, you can create a pdf of all frames with ros2 run tf2_tools view_frames And to check the current transformations between two frames via command line, use #Usage: tf2_echo source_frame target_frame ros2 run tf2_ros tf2_echo base_link camera_color_frame ros2 run tf2_ros tf2_echo base_link camera_color_optical_frame In our case, the excerpt of our camera transformations (i.e. the objectives of our calibration) are structured as follows: The coordinate frames of the Realsense camera in ROS are described here: realsense-ros . Please note that in our setup, the root of the camera node (which also defines its global pose) is the TF camera_bottom_screw_frame . The perspective projection of the camera is computed wrt camera_color_optical_frame . This may be different for your setup, of course.","title":"ROS2 TF preliminaries"},{"location":"offline_hand_eye/#extract-frames","text":"For calibration, we rigidly attach an Apriltag to our robot's endeffector and record sample images and corresponding TFs. The tag can be printed from this pdf: img/tag36h11_0_140mm.pdf or generated with this online generator . Make sure to print with actual size and note the size of the marker (for the tag in this repo, it is 0.14 meters). Next, start the robot and camera ros2 launch demo rviz_demo_launch.py controller_type:=real enable_realsense_camera:=true and record sample frames: ros2 run offline_hand_eye record_calib_data This will open the following GUI where parameters have been adapted from easy_handeye2 : Calibration Type : eye_on_base for a static camera, eye_in_hand when camera is attached to the robot arm Camera Image Topic : ROS topic with the color image of your camera Camera Info Topic : ROS topic with intrinsic parameters of your camera Robot Base Frame : the base or world frame of your environment Robot Effector Frame : the TF frame of the robot's end effector Tracking Base Frame : the TF frame of the camera, markers are reconstructed wrt to this Tracking Marker Frame : optional, the TF of detected markers if these markers are detected online while extracting. Recording Directory : all extracted frames will be stored here in pickle format. Once you have set or loaded these parametes, you can start to move the robot arm around and record some frames. Recording can be done continuously (not recommended at the moment, see issue below) or in single shots. Rule of thumb is to record 12 frames or more. Make sure that the calibration target is moved across a large part of the camera image, shows variation in distance to the camera and significant variations in orientation, as e.g. here: Make sure to save your configuration parameters! Default location for this file is in the root of your repo with filename handeye_calibration_params.json . You may also use a recording and run record_calib_data from the replay (but make sure that the TFs are all received properly): ros2 bag record --all ros2 bag play <your_recording> Timing Issue : When querying TFs for the time stamp of the current image, we often do not receive valid results in our current environment. As a workaround for this problem, we acquire frames only after the robot is standing completely still, i.e. move the robot to a new position, wait shortly and make sure that rviz shows the correct setup, record sample frame, repeat. That way, we can safely utilize the latest TFs, cf. record_calib_data.py . This should be fixed!","title":"Extract frames"},{"location":"offline_hand_eye/#calibrate","text":"The calibration process itself does not need ROS! So we can focus on the actual computation (almost any version of opencv and numpy) and debugging. To install a virtual environment and a jupyter kernel, use: conda create --name offline-hand-eye python=3.12 ipython jupyter conda-forge::matplotlib conda activate offline-hand-eye pip3 install opencv-python conda install conda-forge::apriltag python -m ipykernel install --user --name=offline-hand-eye This has been tested with miniconda, but should also work with venv. A good starting point for understanding the calibration process is the Jupyter notebook in . A more comfortable version of the calibration is using a GUI where you can check, select, and exclude different frames for calibration: conda activate offline-hand-eye cd scripts ./offline_hand_eye_calibration_gui.py --data_path ../doc/sample_data/calibration/calibdata_2025_08_11-11_51_22 --config ../../../handeye_calibration_params.json --output ../../../handeye_calibration.json","title":"Calibrate"},{"location":"offline_hand_eye/#publish-results","text":"After the calibration has been computed, you can publish the resulting TFs. Start the environment: ros2 launch demo rviz_demo_launch.py controller_type:=real enable_realsense_camera:=true and publish the calibrated TFs: ros2 run offline_hand_eye calib_publisher --ros-args -p calibration_file:=handeye_calibration.json Now, run ros2 run tf2_ros tf2_echo base_link camera_bottom_screw_frame to print the extrinsic calibration in the console, e.g. At time 0.0 - Translation: [0.659, 0.459, 1.550] - Rotation: in Quaternion [-0.604, 0.696, 0.293, 0.254] - Rotation: in RPY (radian) [2.999, 0.787, -1.771] - Rotation: in RPY (degree) [171.830, 45.092, -101.471] - Matrix: -0.140 -0.990 0.000 0.659 -0.692 0.098 0.715 0.459 -0.708 0.100 -0.699 1.550 0.000 0.000 0.000 1.000 You may also use this translation and rotation in RPY (radian) to adjust the camera transform in .","title":"Publish results"},{"location":"offline_hand_eye/#for-future-use-running-tag-detectors-online","text":"To run the apriltag detector online, use: sudo apt install ros-jazzy-apriltag-ros ros2 run apriltag_ros apriltag_node --ros-args \\ -r image_rect:=/camera/camera/color/image_raw \\ -r camera_info:=/camera/camera/color/camera_info \\ --params-file /workspace/src/offline_hand_eye/doc/tags_36h11.yaml Or for the aruco marker: sudo apt install ros-jazzy-aruco-ros ros2 run aruco_ros single --ros-args -p image_is_rectified:=true -p marker_size:=0.1 -p marker_id:=1 -p reference_frame:=camera_link -p camera_frame:=/camera/camera/color/image_raw -p marker_frame:=camera_marker -p corner_refinement:=LINES","title":"For future use: running tag detectors online"},{"location":"planner_evaluation/","text":"Planner Evaluation \u00b6 We have evaluated several planners for their applicability with the Cobot. RRTConnect : the default choice in MoveIt2; fast and reliable. Creates two Rapidly-exploring Random Trees (RRTs) - one from the start, one from the goal \u2014 and tries to connect them. SBL : Single-query Bi-directional Lazy planner. Plans (lazy) without collision checking validates after the path was found. LBKPIECE : Uses projection evaluation (joint dimension reduction, refer this page for more details). OMPL config: projection_evaluator . AnyPathShortening : a (meta) planner that runs several planners in parallel. Although AnyPathShortening seems like a good approach, it is slower than a single planner and although we defined 6 different planners, observations revealed that in most cases only SBL , LBKPIECE or RRTConnect were used by AnyPathShortening . A comparison further revealed that SBL yields the best results in our operation domain (the small table attached to the Cobot in direction x+). Evaluation Setup \u00b6 We have defined a bounding box (representing the operation domain) around the work bench and filled the bound box with points; using a resolution of kDelta = 0.05 # 5 [cm] . kXMin, kXMax = 0.3, 0.65 kYMin, kYMax = -0.3, 0.1 kZMin, kZMax = 0.5, 1.0 Point Cloud image of workbench: For each (randomly selected) point in the bounding box, we ran the SBL , LIBPIECE and RRTConnect planner using the ik_reachable_set script; that plans a path with inverse kinematic (IK) and displays successful and failed runs. Each planner had a timeout of 0.5 seconds. Goal was to identify on how many points can be approach by the Cobot from above (for a pick and place task for example). We made sure that an IK solution was not just successful because point were located next to each other by selecting points from the bounding box randomly. Example (green: successful IK runs, red: failed IK runs) Results \u00b6 The final \"point cloud\" for the SBL planner is shown in the following. We can see that most points are accessible and the missing points are located at the edges of the bounding box and result from physical limitations (some are out of reach of the Cobot). Example: even with joint 3 and 4 extracted, the configuration with the longest reach of the cobot, the points in the right corner imply difficulties to approach from above. Projecting the points on the workbench shows that the reach is acceptable. While all three planners yield similar results, SBL seemed to have found more solutions overall. A (visual) comparison of successful and failed IK runs for SBL and RRTConnect (representing the default planner) is shown in the following. An comparison of the points that were only found by SBL or RRTConnect yields that these belong to the left corner, an areal difficult to reach due to the physical characteristics of the Cobot. => Selecting SBL due to the fact, that it found more solutions for the corners.","title":"Planner Evaluation"},{"location":"planner_evaluation/#planner-evaluation","text":"We have evaluated several planners for their applicability with the Cobot. RRTConnect : the default choice in MoveIt2; fast and reliable. Creates two Rapidly-exploring Random Trees (RRTs) - one from the start, one from the goal \u2014 and tries to connect them. SBL : Single-query Bi-directional Lazy planner. Plans (lazy) without collision checking validates after the path was found. LBKPIECE : Uses projection evaluation (joint dimension reduction, refer this page for more details). OMPL config: projection_evaluator . AnyPathShortening : a (meta) planner that runs several planners in parallel. Although AnyPathShortening seems like a good approach, it is slower than a single planner and although we defined 6 different planners, observations revealed that in most cases only SBL , LBKPIECE or RRTConnect were used by AnyPathShortening . A comparison further revealed that SBL yields the best results in our operation domain (the small table attached to the Cobot in direction x+).","title":"Planner Evaluation"},{"location":"planner_evaluation/#evaluation-setup","text":"We have defined a bounding box (representing the operation domain) around the work bench and filled the bound box with points; using a resolution of kDelta = 0.05 # 5 [cm] . kXMin, kXMax = 0.3, 0.65 kYMin, kYMax = -0.3, 0.1 kZMin, kZMax = 0.5, 1.0 Point Cloud image of workbench: For each (randomly selected) point in the bounding box, we ran the SBL , LIBPIECE and RRTConnect planner using the ik_reachable_set script; that plans a path with inverse kinematic (IK) and displays successful and failed runs. Each planner had a timeout of 0.5 seconds. Goal was to identify on how many points can be approach by the Cobot from above (for a pick and place task for example). We made sure that an IK solution was not just successful because point were located next to each other by selecting points from the bounding box randomly. Example (green: successful IK runs, red: failed IK runs)","title":"Evaluation Setup"},{"location":"planner_evaluation/#results","text":"The final \"point cloud\" for the SBL planner is shown in the following. We can see that most points are accessible and the missing points are located at the edges of the bounding box and result from physical limitations (some are out of reach of the Cobot). Example: even with joint 3 and 4 extracted, the configuration with the longest reach of the cobot, the points in the right corner imply difficulties to approach from above. Projecting the points on the workbench shows that the reach is acceptable. While all three planners yield similar results, SBL seemed to have found more solutions overall. A (visual) comparison of successful and failed IK runs for SBL and RRTConnect (representing the default planner) is shown in the following. An comparison of the points that were only found by SBL or RRTConnect yields that these belong to the left corner, an areal difficult to reach due to the physical characteristics of the Cobot. => Selecting SBL due to the fact, that it found more solutions for the corners.","title":"Results"},{"location":"quickstart/","text":"Quickstart Guide \u00b6 Follow the instructions below to plan and execute trajectories in simulation or on the real Cobot. Common Steps \u00b6 1. Clone this Repo \u00b6 git clone https://github.com/robgineer/cobot.git cobot cd ~/cobot 2. Install docker \u00b6 The entire dev. environment is based on a docker container. If you running Ubuntu and you don't have docker installed, run the following script. ./scripts/docker_installation.sh 3. Choose your GUI \u00b6 We offer two different options for displaying the graphical user interface: xpra or VNC. Both are different and have their specific use cases. The VNC option enables viewing an entire Ubuntu Desktop within your browser. Useful in case you are running this project on a local machine. Refer VCN setup for details and configuration. The xpra option enables forwarding single X11 windows from your terminal. Its a bit more lightweight and useful if you are running this project on a remote machine. It requires your user to be ported into the docker container, however. This might not work on all configurations. Refer Xpra setup for details and configuration. Once you have configured your preferred GUI type, proceed with either the simulation or real control steps. Simulation \u00b6 1. Build the project \u00b6 source /opt/ros/jazzy/setup.{bash/zsh} cd ~/cobot colcon build --merge-install --symlink-install --cmake-args \"-DCMAKE_BUILD_TYPE=Release\" --packages-select cobot_model cobot_moveit_config demo py_demo source install/setup.{bash/zsh} Colcon build options explained \u00b6 merge-install: Do not create separate install folders for each package bur merge everything into one. symlink-install: Do not copy built files into the install folders but create a symlink. Changes in Python files, for example, take effect immediately (no re-building required). cmake-args \"-DCMAKE_BUILD_TYPE=Release\": Optimize build for speed --packages-select cobot_model cobot_moveit_config demo py_demo: We only need these packages for a simulation. 2. Launch Moveit2 and ROS2 Control \u00b6 Option A: Fake Controls \u00b6 This is an rviz only setup that enables to visualize Cobot motions based on fake controls; omitting any physics. ros2 launch demo rviz_demo_launch.py Option B: Gazebo Controls \u00b6 This setup enables to visualize requested trajectories in rviz and in Gazebo. The Gazebo part will consider physical characteristics of the model such as mass and inertia. ros2 launch cobot_moveit_config gz_demo_launch.py 3. Control the Cobot in Simulation \u00b6 You can either drag the Cobot's end effector in rviz to a preferred position via the MoveIt2 motion planning plugin and execute the resulting trajectory or run a demo. Example: ros2 run demo simple_ik or (fake controllers only) ros2 run py_demo simple_ik_fk Real Cobot Control \u00b6 Setup overview: 1. Pull submodules \u00b6 In order to manipulate the Cobot in ROS2, connect to HS-Esslingen VPN, log in into the dedicated workstation for the Cobot and run source /opt/ros/jazzy/setup.{bash/zsh} cd ~/cobot git submodule init src/cobot_hardware src/realsense-ros git submodule update src/cobot_hardware src/realsense-ros to pull the submodules. Build with: colcon build --merge-install --symlink-install --cmake-args \"-DCMAKE_BUILD_TYPE=Release\" --packages-skip moveit_task_constructor => build everything except of the moveit_task_constructor (not required for the basic tasks). Don't forget to source source install/setup.{bash/zsh} Note if you are not connected to the HS-Esslingen VPN, the cobot_hardware submodule will not be cloned and you will not be able to control the Cobot via MoveIt2 / ROS2. 2. Launch Moveit2 and ROS2 Control \u00b6 Launch rviz_demo_launch.py with controller_type:=real and enable_realsense_camera:=true ros2 launch demo rviz_demo_launch.py controller_type:=real enable_realsense_camera:=true 3. Control the Cobot \u00b6 Same as in simulation, you can either drag the Cobot's end effector in rviz to a preferred position via the MoveIt2 motion planning plugin and execute the resulting trajectory or run a demo. Example: ros2 run demo simple_ik How to handle rogue grippers \u00b6 Once activated, the vacuum system remains active; even if the cobot is shut down physically (the red emergency button on the cobot table). In order to deactivate the vacuum system, run ros2 run cobot_hardware gripper_off","title":"Quickstart"},{"location":"quickstart/#quickstart-guide","text":"Follow the instructions below to plan and execute trajectories in simulation or on the real Cobot.","title":"Quickstart Guide"},{"location":"quickstart/#common-steps","text":"","title":"Common Steps"},{"location":"quickstart/#1-clone-this-repo","text":"git clone https://github.com/robgineer/cobot.git cobot cd ~/cobot","title":"1. Clone this Repo"},{"location":"quickstart/#2-install-docker","text":"The entire dev. environment is based on a docker container. If you running Ubuntu and you don't have docker installed, run the following script. ./scripts/docker_installation.sh","title":"2. Install docker"},{"location":"quickstart/#3-choose-your-gui","text":"We offer two different options for displaying the graphical user interface: xpra or VNC. Both are different and have their specific use cases. The VNC option enables viewing an entire Ubuntu Desktop within your browser. Useful in case you are running this project on a local machine. Refer VCN setup for details and configuration. The xpra option enables forwarding single X11 windows from your terminal. Its a bit more lightweight and useful if you are running this project on a remote machine. It requires your user to be ported into the docker container, however. This might not work on all configurations. Refer Xpra setup for details and configuration. Once you have configured your preferred GUI type, proceed with either the simulation or real control steps.","title":"3. Choose your GUI"},{"location":"quickstart/#simulation","text":"","title":"Simulation"},{"location":"quickstart/#1-build-the-project","text":"source /opt/ros/jazzy/setup.{bash/zsh} cd ~/cobot colcon build --merge-install --symlink-install --cmake-args \"-DCMAKE_BUILD_TYPE=Release\" --packages-select cobot_model cobot_moveit_config demo py_demo source install/setup.{bash/zsh}","title":"1. Build the project"},{"location":"quickstart/#colcon-build-options-explained","text":"merge-install: Do not create separate install folders for each package bur merge everything into one. symlink-install: Do not copy built files into the install folders but create a symlink. Changes in Python files, for example, take effect immediately (no re-building required). cmake-args \"-DCMAKE_BUILD_TYPE=Release\": Optimize build for speed --packages-select cobot_model cobot_moveit_config demo py_demo: We only need these packages for a simulation.","title":"Colcon build options explained"},{"location":"quickstart/#2-launch-moveit2-and-ros2-control","text":"","title":"2. Launch Moveit2 and ROS2 Control"},{"location":"quickstart/#option-a-fake-controls","text":"This is an rviz only setup that enables to visualize Cobot motions based on fake controls; omitting any physics. ros2 launch demo rviz_demo_launch.py","title":"Option A: Fake Controls"},{"location":"quickstart/#option-b-gazebo-controls","text":"This setup enables to visualize requested trajectories in rviz and in Gazebo. The Gazebo part will consider physical characteristics of the model such as mass and inertia. ros2 launch cobot_moveit_config gz_demo_launch.py","title":"Option B: Gazebo Controls"},{"location":"quickstart/#3-control-the-cobot-in-simulation","text":"You can either drag the Cobot's end effector in rviz to a preferred position via the MoveIt2 motion planning plugin and execute the resulting trajectory or run a demo. Example: ros2 run demo simple_ik or (fake controllers only) ros2 run py_demo simple_ik_fk","title":"3. Control the Cobot in Simulation"},{"location":"quickstart/#real-cobot-control","text":"Setup overview:","title":"Real Cobot Control"},{"location":"quickstart/#1-pull-submodules","text":"In order to manipulate the Cobot in ROS2, connect to HS-Esslingen VPN, log in into the dedicated workstation for the Cobot and run source /opt/ros/jazzy/setup.{bash/zsh} cd ~/cobot git submodule init src/cobot_hardware src/realsense-ros git submodule update src/cobot_hardware src/realsense-ros to pull the submodules. Build with: colcon build --merge-install --symlink-install --cmake-args \"-DCMAKE_BUILD_TYPE=Release\" --packages-skip moveit_task_constructor => build everything except of the moveit_task_constructor (not required for the basic tasks). Don't forget to source source install/setup.{bash/zsh} Note if you are not connected to the HS-Esslingen VPN, the cobot_hardware submodule will not be cloned and you will not be able to control the Cobot via MoveIt2 / ROS2.","title":"1. Pull submodules"},{"location":"quickstart/#2-launch-moveit2-and-ros2-control_1","text":"Launch rviz_demo_launch.py with controller_type:=real and enable_realsense_camera:=true ros2 launch demo rviz_demo_launch.py controller_type:=real enable_realsense_camera:=true","title":"2. Launch Moveit2 and ROS2 Control"},{"location":"quickstart/#3-control-the-cobot","text":"Same as in simulation, you can either drag the Cobot's end effector in rviz to a preferred position via the MoveIt2 motion planning plugin and execute the resulting trajectory or run a demo. Example: ros2 run demo simple_ik","title":"3. Control the Cobot"},{"location":"quickstart/#how-to-handle-rogue-grippers","text":"Once activated, the vacuum system remains active; even if the cobot is shut down physically (the red emergency button on the cobot table). In order to deactivate the vacuum system, run ros2 run cobot_hardware gripper_off","title":"How to handle rogue grippers"}]}